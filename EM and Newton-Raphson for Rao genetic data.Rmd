---
title: "EM and Newton-Raphson for Rao genetic data"
author: "Roger Day"
date: "October 5, 2015"
output: html_document
---

Animals that are crosses of AB/ab x  AB/ab are classified in 4 categories: 
\[\begin{array}{*{20}{c}}
  Y&{Genotype}&{\Pr (category|\theta )}&{\Pr (category|\pi )} \\ 
  {125}&{AB}&{(3 - 2\theta  + {\theta ^2})/4}&{\frac{1}{2} + \frac{\pi }{4}} \\ 
  {18}&{Ab}&{(2\theta  - {\theta ^2})/4}&{\frac{1}{4} - \frac{\pi }{4}} \\ 
  {20}&{aB}&{(2\theta  - {\theta ^2})/4}&{\frac{1}{4} - \frac{\pi }{4}} \\ 
  {34}&{ab}&{(1 - 2\theta  + {\theta ^2})/4}&{\frac{\pi }{4}} 
\end{array}\]
The data are:
```{r }
y <- y.rao <- c(125,18,20,34)  #Original data.

```

The likelihood function is:
\[\begin{gathered}
  ODL \propto [Y|\pi ] \propto {\left( {\frac{1}{2} + \frac{\pi }{4}} \right)^{{Y_1}}}{\left( {\frac{1}{4} - \frac{\pi }{4}} \right)^{{Y_2} + {Y_3}}}{\left( {\frac{\pi }{4}} \right)^{{Y_4}}}  \\
  \quad \quad \quad \quad \quad \quad  \propto \sum\limits_{{X_1} = 0}^{{Y_1}} {\frac{{n!}}{{{X_1}!{X}!{Y_2}!{Y_3}!{Y_4}!}}{{\left( {\frac{1}{2}} \right)}^{{X_1}}}{{\left( {\frac{1}{4} - \frac{\pi }{4}} \right)}^{{Y_2} + {Y_3}}}{{\left( {\frac{\pi }{4}} \right)}^{{X} + {Y_4}}}}   \\
  \quad \quad \quad \quad = \sum\limits_{{X_1} = 0}^{{Y_1}} {\frac{{n!}}{{{X_1}!{X}!{Y_2}!{Y_3}!{Y_4}!}}CDL(\pi ;{X_1},{X},{Y_2},{Y_3},{Y_4})}   \\ 
\end{gathered} \]

The code for the likelihood function and its log:
```{r }
odl <- odl.rao <- function(y,p)
{
  (1/2+p/4)^y[1] * ((1-p)/4)^(y[2]+y[3]) * (p/4)^y[4] 
}

log.odl <- function(y,p) {
  log(odl(y,p))
}

log.cdl <- log.cdl.rao <- function(ExpX, y, p)
{
  if(length(p) > 1)
    return(sapply(p, log.cdl, ExpX=ExpX, y=y))
  log(dmultinom(
    x=c(y[1]-ExpX, ExpX, y[2], y[3], y[4]), 
    prob = c(1/2, p/4, (1-p)/4, (1-p)/4, p/4))
  )
}

```
Plots of the ODL and its log:
```{r}
p.delta <- 0.01
p.range <- seq(.01,.99,by=p.delta)
plot (p.range, odl(y,p.range), type="l",
      main="likelihood function")
plot (p.range, log.odl(y,p.range), type="l",
      main="log(likelihood)")

```

Newton-Raphson method uses the first and second derivatives:
```{r}
logodlDot = function(y,p)
  y[1]/(2+p) -(y[2]+y[3])/(1-p) + y[4]/p
logodlDotDot = function(y,p)
  -(y[1]/(2+p)^2 + (y[2]+y[3])/(1-p)^2 + y[4]/p^2)

plot (p.range, log.odl(y,p.range), type="l",
      main="log(likelihood);  Newton-Raphson")
p0 = 0.01
for(iter in 1:9) {
  points(p0, log(odl.rao(y=y.rao, p0)), col="red", cex=4, pch=as.character(iter))
  p0 = p0 - logodlDot(y.rao, p0)/logodlDotDot(y.rao, p0)
}
p0 = 0.99
for(iter in 1:9) {
  points(p0, log(odl.rao(y=y.rao, p0)), col="blue", cex=4, pch=as.character(iter))
  p0 = p0 - logodlDot(y.rao, p0)/logodlDotDot(y.rao, p0)
}
```

For the EM algorithm, we need  the complete data likelihood. The complete data would also contain $X$ the portion of the first category produced by the
${\frac{\pi }{4}}$ portion.
\[\begin{gathered}
  CDL = [X|\pi ] = \frac{{n!}}{{({Y_1} - X)!X!{Y_2}!{Y_3}!{Y_4}!}}{\left( {\frac{1}{2}} \right)^{{Y_1} - X}}{\left( {\frac{\pi }{4}} \right)^{{X}}}{\left( {\frac{1}{4} - \frac{\pi }{4}} \right)^{{Y_2} + {Y_3}}}{\left( {\frac{\pi }{4}} \right)^{{Y_4}}}  \\
\end{gathered} \]
or, simply,
\[
CDL \propto {\pi ^{{X} + {Y_4}}}{(1 - \pi )^{{Y_2} + {Y_3}}} \\
  \quad \quad \quad \quad  \\ 
\]
and the  expected sufficient statistic,

\[E({X}|Y,\pi ) = {Y_1}\frac{\pi / 4 }{{\pi / 4 + 1/2 }} = {Y_1}\frac{\pi }{{ \pi + 2 }}\]

## The likelihood function,  and the EM:
```{r}
plot (p.range, log.odl(y,p.range), type="l",
      main="log(likelihood);  EM algorithm")
## the "M" function 
M = function(phi){
  ExpX <<-  y[1]*phi/(2+phi) 
  return(
    (ExpX + y[4])/(ExpX + y[2]+y[3] + y[4])
  )
}

p0star = p0 = 0.01
nsteps = 9
em = function(p0=0.01, nsteps=9, tol = 1e-9) {
  results = data.frame(iter=NA, ExpX=NA, p0=NA,
                       p0change=NA, logodl=NA)
  iter = 1
  options(digits=12)
  repeat {
    p0star = p0
    points(p0, log(odl.rao(y=y.rao, p0)), col="red", cex=4, pch=as.character(iter))
    p0 = M(p0)
    results[iter, ] = data.frame(
      iter=iter, 
      ExpX=ExpX, p0=p0,
      p0change=abs(p0 - p0star), 
      logodl=log.odl(y, p0))
    if(abs(p0 - p0star) < tol)
      break
    iter = iter + 1
    if(iter > nsteps)
      break
  }
  results
}
em( p0=0.01)
em.output = print(em( p0=0.99))
ExpX = em.output[nrow(em.output), "ExpX"]
cat("ExpX is: ", ExpX, "\n")
p0 = em.output[nrow(em.output), "p0"]
```

By reputation, Newton-Raphson converges quickly, and EM slowly.  Here, we see the opposite.

## The score (derivative of log likelihood) and the information matrix (-1 times 2nd derivative).
If ExpX is missing, then we return the score for the log ODL; otherwise the score for the log CDL. Likewise for the information.

```{r}


#cat("ExpX is: ", ExpX, "\n")

scorefun = function(ExpX, p0, delta = 1e-8, verbose=FALSE) {
  if(missing(ExpX)) {
    if(verbose)
      print( ( log.odl(y, p0) - 
            log.odl(y, p0-delta) ) / delta)
    return( - (y[2]+y[3])/(1-p0)  + y[1] / (2+p0) 
        + y[4]/p0)
  }
  if(verbose)
    print( ( log.cdl(ExpX, y, p0) 
        - log.cdl(ExpX, y, p0-delta) ) / delta)
  return( - (y[2]+y[3])/(1-p0)  + (ExpX+y[4]) / p0 )
}
scorefun(p=p0)
scorefun(ExpX=ExpX, p=p0)

infofun = function(ExpX, p0) {
  if(missing(ExpX)) {
    return( (y[2]+y[3])/(1-p0)^2 + y[1] / (2+p0)^2 
        + y[4]/p0^2)
  }
  if(length(ExpX) > 1)
    return(sapply(ExpX, infofun, p0=p0))
  return(  (y[2]+y[3])/(1-p0)^2  
           + (ExpX+y[4]) / p0^2 )
}
cat("Observed information: ", infofun(p=p0), "\n")
cat("Complete information: ", infofun(ExpX=ExpX, p=p0), "\n")
cat("Slope should be approximately",
  Mslope <<- 1 - infofun(p=p0)/infofun(ExpX=ExpX, p=p0), "\n")
sampleOfX = rbinom(100000, y[1], p0/(2+p0)) 
cat("Expected complete information: ",
  expectedCompleteInfo <- mean(sapply(sampleOfX, infofun, p=p0)), "\n")
Mslope <<- 1 - infofun(p=p0)/expectedCompleteInfo 
cat("Slope should be exactly", Mslope, "\n")
```
Finally, we plot the M function:
```{r}
plot(0:1,0:1, pch="")
abline(a=0, b=1)
points(p.range, M(p.range))
abline(v=p0, lty=2, col="green")
print(p0)
abline(a = p0 - Mslope*p0, b = Mslope, col="blue")

p0 = 0.01
for(i in 1:4)
points(p0, p0<-M(p0), col="red", pch=as.character(i),
cex=3)
```

Thus the slope at the convergence point is as "Amazing  Fact (o)" says.

#### Standard error of the estimate.

The information computations are useful for getting a standard error and confidence intervals for the parameter.
The inverse of the information is:
```{r}
1/infofun(p=p0)
```
so a confidence interval for the estimate $\pi$ = `r round(p0, 4)` is 
```{r}
conf.int.untransformed <- print(round(digits=3, p0 + c(1,-1) * qnorm(0.025) * sqrt(1/infofun(p=p0))))
```
This interval is symmetric in the untransformed scale of probability. We know we can do better on a transformed scale that respects the boundaries of the unit interval. Let's try the logit transformation. We take into account the Jacobean in revising the observed information.
```{r}
logit = function(p) log(p/(1-p))
antilogit = function(z) 1 - 1/(1+exp(z))
# CHECK: print(antilogit(logit(1/3)))
jacobean.dz.dp =  function(p) 1/p + 1/(1-p)
obs.info.wrt.z = infofun(p=p0) / (jacobean.dz.dp(p0))^2
conf.int.transformed <-
  print(round(digits=3, antilogit(
    logit(p0) + c(1,-1) * qnorm(0.025) * 
      sqrt(1/obs.info.wrt.z ))))
```
This is slightly shifted to the left, which is appropriate given the shape of the log likelihood function.
```{r}
plot (p.range[50:80], log.odl(y,p.range[50:80]), type="l",
      main="log(likelihood)", xlim = c(0.5, 0.8))
points(p0, log.odl(y,p0), pch=6, cex=2)
lines(conf.int.untransformed, log.odl(y,conf.int.untransformed), col="red", pch=7, type='b')
lines(conf.int.transformed, log.odl(y,conf.int.transformed), col="blue", pch=8, type='b')
legend("bottomleft", col = c('black','red','blue'), pch = c(6,7,8), lty=c(0,1,1),
       legend = c("MLE", 'conf.int.untransformed', 'conf.int.transformed'))

```
Here are the log likelihoods at the endpoints:

Untransformed interval: `r round(digits=3, log.odl(y,conf.int.untransformed))` 

Transformed interval: `r round(digits=3,log.odl(y,conf.int.transformed))`.

The second  is much closer to equal log-likelihoods.


